"""
Model Validation Framework

Compares simulation outputs against historical SCS incidents to assess model accuracy.

Based on:
- Sargent (2013): Verification and Validation of Simulation Models
- KlÃ¼gl (2008): A Validation Methodology for Agent-Based Simulations
"""

from __future__ import annotations
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
import pandas as pd
import numpy as np
from dataclasses import dataclass
from scipy import stats

from scs_mediator_sdk.sim.enhanced_abm import ConflictSimulation


@dataclass
class ValidationMetrics:
    """Validation results comparing model vs reality"""
    incident_frequency_rmse: float
    severity_correlation: float
    escalation_pattern_match: float
    overall_accuracy: float
    confidence_interval: Tuple[float, float]


class ModelValidator:
    """Validates simulation model against historical data"""

    def __init__(self, historical_data_path: str = None):
        if historical_data_path is None:
            # Default to bundled historical data
            historical_data_path = Path(__file__).parent.parent.parent.parent / "data" / "historical_scs_incidents.json"

        with open(historical_data_path) as f:
            data = json.load(f)
            self.historical_incidents = data['incidents']
            self.statistics = data.get('aggregate_statistics', {})

    def run_validation(self,
                      n_simulations: int = 100,
                      steps_per_sim: int = 200) -> ValidationMetrics:
        """
        Run model validation against historical data

        Args:
            n_simulations: Number of simulation runs for statistical validity
            steps_per_sim: Steps per simulation (200 steps â‰ˆ 12 months in model)

        Returns:
            ValidationMetrics with accuracy scores
        """
        print(f"Running {n_simulations} validation simulations...")

        # Run multiple simulations
        sim_results = []
        for i in range(n_simulations):
            sim = ConflictSimulation(
                steps=steps_per_sim,
                domain="maritime",
                environment={"weather_bad": False, "media_visibility": 2},
                agreement=None,  # No agreement (like historical period)
                seed=i
            )
            df = sim.run()
            sim_results.append(df)

        # Compare against historical data
        metrics = self._calculate_validation_metrics(sim_results)

        print(f"\nâœ… Validation complete!")
        print(f"   Overall Accuracy: {metrics.overall_accuracy:.1%}")
        print(f"   Incident Frequency RMSE: {metrics.incident_frequency_rmse:.3f}")
        print(f"   Severity Correlation: {metrics.severity_correlation:.3f}")

        return metrics

    def _calculate_validation_metrics(self, sim_results: List[pd.DataFrame]) -> ValidationMetrics:
        """Calculate validation metrics"""

        # 1. Incident Frequency
        sim_frequencies = [len(df) for df in sim_results]
        historical_frequency = len(self.historical_incidents)

        # Normalize to same time period (historical is 12 years, sim is 200 steps â‰ˆ 1 year)
        historical_per_year = historical_frequency / 12
        avg_sim_per_year = np.mean(sim_frequencies)

        frequency_rmse = np.sqrt((avg_sim_per_year - historical_per_year) ** 2)

        # 2. Severity Distribution
        historical_severities = [inc['severity'] for inc in self.historical_incidents]
        sim_severities = [sev for df in sim_results for sev in df['severity']]

        if len(sim_severities) > 0 and len(historical_severities) > 0:
            # Correlation between distributions
            hist_bins = np.histogram(historical_severities, bins=10, range=(0,1))[0]
            sim_bins = np.histogram(sim_severities, bins=10, range=(0,1))[0]

            severity_corr = stats.pearsonr(hist_bins, sim_bins)[0] if len(hist_bins) > 1 else 0.5
        else:
            severity_corr = 0.0

        # 3. Escalation Pattern Matching
        # Check if model reproduces pattern of incident types
        historical_types = {}
        for inc in self.historical_incidents:
            incident_type = inc['incident_type']
            historical_types[incident_type] = historical_types.get(incident_type, 0) + 1

        sim_types = {}
        for df in sim_results:
            for itype in df['incident_type']:
                sim_types[itype] = sim_types.get(itype, 0) + 1

        # Calculate distribution similarity (Chi-square goodness of fit)
        pattern_match = self._calculate_distribution_similarity(historical_types, sim_types)

        # 4. Overall Accuracy (weighted combination)
        frequency_score = max(0, 1 - (frequency_rmse / historical_per_year))
        severity_score = max(0, (severity_corr + 1) / 2)  # Convert correlation to 0-1
        pattern_score = pattern_match

        overall = (frequency_score * 0.3 + severity_score * 0.4 + pattern_score * 0.3)

        # 5. Confidence Interval (bootstrap)
        ci_lower, ci_upper = self._bootstrap_confidence_interval(sim_frequencies, historical_per_year)

        return ValidationMetrics(
            incident_frequency_rmse=frequency_rmse,
            severity_correlation=severity_corr,
            escalation_pattern_match=pattern_match,
            overall_accuracy=overall,
            confidence_interval=(ci_lower, ci_upper)
        )

    def _calculate_distribution_similarity(self,
                                          hist_dist: Dict[str, int],
                                          sim_dist: Dict[str, int]) -> float:
        """Calculate how similar two distributions are (0-1)"""
        if not hist_dist or not sim_dist:
            return 0.5

        # Normalize to probabilities
        hist_total = sum(hist_dist.values())
        sim_total = sum(sim_dist.values())

        all_types = set(list(hist_dist.keys()) + list(sim_dist.keys()))

        hist_probs = [hist_dist.get(t, 0) / hist_total for t in all_types]
        sim_probs = [sim_dist.get(t, 0) / sim_total for t in all_types]

        # Use Jensen-Shannon divergence (symmetric, bounded 0-1)
        from scipy.spatial.distance import jensenshannon
        divergence = jensenshannon(hist_probs, sim_probs)

        # Convert divergence (0 = identical, 1 = completely different) to similarity
        similarity = 1 - divergence

        return max(0, min(1, similarity))

    def _bootstrap_confidence_interval(self,
                                       samples: List[float],
                                       true_value: float,
                                       confidence: float = 0.95) -> Tuple[float, float]:
        """Calculate bootstrap confidence interval"""
        n_bootstrap = 1000
        bootstrap_diffs = []

        for _ in range(n_bootstrap):
            resample = np.random.choice(samples, size=len(samples), replace=True)
            diff = np.mean(resample) - true_value
            bootstrap_diffs.append(diff)

        alpha = 1 - confidence
        lower = np.percentile(bootstrap_diffs, alpha/2 * 100)
        upper = np.percentile(bootstrap_diffs, (1 - alpha/2) * 100)

        return (true_value + lower, true_value + upper)

    def generate_validation_report(self, metrics: ValidationMetrics) -> str:
        """Generate human-readable validation report"""

        report = f"""
# Model Validation Report

## Overview
This report compares the SCS Mediator Simulation Model against historical South China Sea incidents (2012-2024).

## Validation Metrics

### 1. Incident Frequency
- **RMSE**: {metrics.incident_frequency_rmse:.3f}
- **Interpretation**: {"âœ… GOOD" if metrics.incident_frequency_rmse < 1.0 else "âš ï¸ NEEDS CALIBRATION"}
- Model predicts incident frequency within acceptable range of historical data.

### 2. Severity Distribution
- **Correlation**: {metrics.severity_correlation:.3f}
- **Interpretation**: {"âœ… STRONG" if metrics.severity_correlation > 0.7 else "âš ï¸ MODERATE" if metrics.severity_correlation > 0.4 else "âŒ WEAK"}
- Model's severity distribution {"matches" if metrics.severity_correlation > 0.7 else "somewhat matches" if metrics.severity_correlation > 0.4 else "differs from"} historical patterns.

### 3. Escalation Pattern Matching
- **Similarity Score**: {metrics.escalation_pattern_match:.1%}
- **Interpretation**: {"âœ… EXCELLENT" if metrics.escalation_pattern_match > 0.8 else "âœ… GOOD" if metrics.escalation_pattern_match > 0.6 else "âš ï¸ FAIR"}
- Model reproduces the types and patterns of incidents observed historically.

### 4. Overall Accuracy
- **Score**: {metrics.overall_accuracy:.1%}
- **Confidence Interval**: ({metrics.confidence_interval[0]:.2f}, {metrics.confidence_interval[1]:.2f})
- **Interpretation**: {"âœ… MODEL VALIDATED" if metrics.overall_accuracy > 0.7 else "âš ï¸ ACCEPTABLE WITH LIMITATIONS" if metrics.overall_accuracy > 0.5 else "âŒ REQUIRES RECALIBRATION"}

## Recommendations

{"### âœ… Model is ready for training use" if metrics.overall_accuracy > 0.7 else "### âš ï¸ Model should be used with caution"}

**Strengths:**
{self._identify_strengths(metrics)}

**Limitations:**
{self._identify_limitations(metrics)}

**Suggested Improvements:**
{self._suggest_improvements(metrics)}

## Methodology

- **Historical Data**: 9 major SCS incidents (2012-2024)
- **Validation Runs**: 100 simulations with different random seeds
- **Comparison Method**: Statistical correlation and distribution matching
- **Confidence Level**: 95%

## References

- Sargent, R.G. (2013). Verification and Validation of Simulation Models
- KlÃ¼gl, F. (2008). A Validation Methodology for Agent-Based Simulations
- CSIS Asia Maritime Transparency Initiative (Historical Data Source)

---
*Generated: {pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
        return report

    def _identify_strengths(self, metrics: ValidationMetrics) -> str:
        strengths = []
        if metrics.incident_frequency_rmse < 1.0:
            strengths.append("- Accurate incident frequency prediction")
        if metrics.severity_correlation > 0.6:
            strengths.append("- Good severity distribution matching")
        if metrics.escalation_pattern_match > 0.6:
            strengths.append("- Realistic escalation patterns")

        return "\n".join(strengths) if strengths else "- Model captures general conflict dynamics"

    def _identify_limitations(self, metrics: ValidationMetrics) -> str:
        limitations = []
        if metrics.incident_frequency_rmse > 1.5:
            limitations.append("- Incident frequency may need calibration")
        if metrics.severity_correlation < 0.5:
            limitations.append("- Severity distribution differs from historical patterns")
        if metrics.escalation_pattern_match < 0.5:
            limitations.append("- Incident type distribution needs improvement")

        limitations.append("- Limited to maritime domain (SCS specific)")
        limitations.append("- Does not model external interventions (US, ASEAN)")

        return "\n".join(limitations)

    def _suggest_improvements(self, metrics: ValidationMetrics) -> str:
        suggestions = []

        if metrics.incident_frequency_rmse > 1.0:
            suggestions.append("- Calibrate agent aggression parameters using parameter_calibrator.py")
        if metrics.severity_correlation < 0.6:
            suggestions.append("- Adjust severity calculation formulas in enhanced_abm.py")
        if metrics.escalation_pattern_match < 0.6:
            suggestions.append("- Review incident type probabilities in _get_default_incident_types()")

        suggestions.append("- Add more historical incidents to data/historical_scs_incidents.json")
        suggestions.append("- Run sensitivity analysis to identify key parameters")

        return "\n".join(suggestions)


# Example usage
if __name__ == "__main__":
    validator = ModelValidator()
    metrics = validator.run_validation(n_simulations=100)
    report = validator.generate_validation_report(metrics)

    # Save report
    output_path = Path(__file__).parent.parent.parent.parent / "docs" / "MODEL_VALIDATION_REPORT.md"
    output_path.parent.mkdir(exist_ok=True)
    with open(output_path, 'w') as f:
        f.write(report)

    print(f"\nðŸ“„ Report saved to: {output_path}")
